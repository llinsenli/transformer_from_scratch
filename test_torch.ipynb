{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "- **Input to `nn.Embedding`**: The input to an `nn.Embedding` layer is a tensor of indices with shape `(..., I)`, where `...` represents any number of preceding dimensions, and `I` is the size of the last dimension, containing the indices.\n",
    "  \n",
    "- **Output of `nn.Embedding`**: The `nn.Embedding` layer replaces each index in its last dimension with an embedding vector of size `E` (where `E` is the embedding size specified when the `nn.Embedding` layer was created). So, for each index in the input tensor, you get an `E`-dimensional vector in the output tensor. This does indeed increase the dimensionality of the input tensor by one.\n",
    "\n",
    "- **Output Shape**: If the input tensor to the `nn.Embedding` layer has a shape of `(..., I)`, the output tensor will have a shape of `(..., I, E)`. Each index in the last dimension of the input tensor is replaced by an `E`-dimensional embedding vector, so the last dimension of the input tensor (`I`) directly corresponds to the second-to-last dimension of the output tensor, and the embedding size `E` becomes the size of the new last dimension.\n",
    "\n",
    "\n",
    "\n",
    "#### Key Difference in Input Types\n",
    "\n",
    "- **`nn.Linear`** expects a tensor of floating-point numbers and applies a linear transformation.\n",
    "- **`nn.Embedding`** expects a tensor of indices (integer values) and retrieves embeddings from an internal lookup table.\n",
    "\n",
    "Despite these differences, the commonality is that both layers transform the last dimension of their input tensor: `nn.Linear` transforms feature vectors, while `nn.Embedding` transforms indices into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_indices:\n",
      " torch.Size([3, 2])\n",
      "Embedding Vectors:\n",
      " torch.Size([3, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding layer\n",
    "batch_size = 4\n",
    "seq_len = 2\n",
    "vocab_size = 10  # Number of words in the vocabulary\n",
    "d_model = 10  # Dimensionality of the embedding vectors\n",
    "embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# Example indices for 2 words from the vocabulary\n",
    "word_indices_1 = torch.tensor([2, 3], dtype=torch.long)\n",
    "word_indices_2 = torch.tensor([3, 6], dtype=torch.long)\n",
    "word_indices_3 = torch.tensor([5, 2], dtype=torch.long)\n",
    "\n",
    "batch_indices = torch.tensor([[2, 3], [3, 6],[5, 2]])\n",
    "\n",
    "\n",
    "# Get the embedding vectors for these words\n",
    "embedding_vectors = embedding_layer(batch_indices)\n",
    "\n",
    "\n",
    "print(\"Batch_indices:\\n\", batch_indices.size())\n",
    "print(\"Embedding Vectors:\\n\", embedding_vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [3, 6],\n",
       "        [5, 2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unsqueeze(0) and unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: torch.Size([3, 4])\n",
      "Shape after unsqueeze(0): torch.Size([1, 3, 4])\n",
      "Shape after unsqueeze(1): torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "'''\n",
    "unsqueeze(0) adds a new dimension at the beginning of the tensor's shape. Often used when you need to add a batch dimension to a tensor for operations that expect batches.\n",
    "unsqueeze(1) adds a new dimension as the second dimension of the tensor's shape.\n",
    "'''\n",
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "print(\"Original tensor shape:\", x.shape)  # Outputs: torch.Size([3, 4])\n",
    "\n",
    "x_unsqueeze_0 = x.unsqueeze(0)\n",
    "print(\"Shape after unsqueeze(0):\", x_unsqueeze_0.shape)  # Outputs: torch.Size([1, 3, 4])\n",
    "\n",
    "x_unsqueeze_1 = x.unsqueeze(1)\n",
    "print(\"Shape after unsqueeze(1):\", x_unsqueeze_1.shape)  # Outputs: torch.Size([3, 1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor with shape (3, 4)\n",
    "a = torch.zeros(3, 4)\n",
    "print(a.squeeze(0).shape)  # Output: torch.Size([3, 4])\n",
    "\n",
    "# Tensor with shape (1, 3, 4)\n",
    "b = torch.zeros(1, 3, 4)\n",
    "print(b.squeeze(0).shape)  # Output: torch.Size([3, 4])\n",
    "\n",
    "# Tensor with shape (1, 3, 1, 4)\n",
    "c = torch.zeros(1, 3, 1, 4)\n",
    "print(c.squeeze().shape)  # Output: torch.Size([3, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  2,  4],\n",
       "        [ 0,  4,  8],\n",
       "        [ 0,  6, 12],\n",
       "        [ 0,  8, 16]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 10, 2) # [5]\n",
    "y = torch.arange(0, 3) # [3]\n",
    "x.unsqueeze(1) * y.unsqueeze(0) # [5, 1] * [1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings x:\n",
      " torch.Size([2, 3, 4])\n",
      "\n",
      "Positional Encodings pe:\n",
      " torch.Size([1, 3, 4])\n",
      "\n",
      "x with Positional Encodings:\n",
      " torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Input embeddings tensor with shape [batch_size, seq_len, d_model]\n",
    "# Here, batch_size=2, seq_len=3, d_model=4\n",
    "x = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0], [3.0, 4.0, 5.0, 6.0]],\n",
    "                  [[4.0, 5.0, 6.0, 7.0], [5.0, 6.0, 7.0, 8.0], [6.0, 7.0, 8.0, 9.0]]])\n",
    "\n",
    "# Simplified positional encodings tensor with shape [1, seq_len, d_model]\n",
    "pe = torch.tensor([[[0.1, 0.2, 0.3, 0.4], [0.2, 0.3, 0.4, 0.5], [0.3, 0.4, 0.5, 0.6]]])\n",
    "\n",
    "x_with_pe = x + pe[:,:x.shape[1] ,:]\n",
    "\n",
    "print(\"Input Embeddings x:\\n\", x.size())\n",
    "print(\"\\nPositional Encodings pe:\\n\", pe.size())\n",
    "print(\"\\nx with Positional Encodings:\\n\", x_with_pe.size())\n",
    "#print(\"\\nx after Dropout:\\n\", x_after_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim = -1, keepdim=True).size() # Take the mean on the last dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "A = torch.rand(2, 4, 3)  # Shape: (batch_size, seq_len, d_model)\n",
    "B = torch.rand(2, 4, 1)  # Shape: (batch_size, seq_len, 1)\n",
    "# Manually broadcasting B to match A's shape\n",
    "d_model = A.shape[2]  # Get the size of the last dimension of A\n",
    "B_expanded = B.expand(-1, -1, d_model)  # Expand B to match A's shape, -1 is used to indicate dimensions that should not be changed, and d_model is the desired size of the last dimension. \n",
    "# Adding the manually broadcasted tensors\n",
    "C = A + B_expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]],\n",
       "\n",
       "        [[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C ==  A+B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear(), nn.Dropout\n",
    "#### `nn.Linear(d_model, d_ff,)`\n",
    "\n",
    "The `nn.Linear` layer, also known as a fully connected or dense layer, performs a linear transformation on the incoming data. It applies a transformation to the input data using a matrix multiplication with its weight matrix and adds a bias.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `d_model`: The size of each input sample (number of input features).\n",
    "  - `d_ff`: The size of each output sample (number of output features).\n",
    "\n",
    "- **Operation**: Given an input `x` of shape `[batch_size, seq_len, d_model]`, the `nn.Linear(d_model, d_ff)` layer transforms `x` to a new shape `[batch_size, seq_len, d_ff]` by applying the following linear transformation:\n",
    "\n",
    "\\[ \\text{output} = x \\cdot W^T + b \\]\n",
    "\n",
    "where:\n",
    "- `x` is the input matrix.\n",
    "- `W` is the weight matrix of the layer (of shape `[d_model, d_ff]`).\n",
    "- `b` is the bias vector (of shape `[d_ff]`).\n",
    "\n",
    "- **Example**:\n",
    "  - If `d_model = 4` and `d_ff = 8`, then the layer will transform input data from 4-dimensional space to 8-dimensional space at each position in the sequence.\n",
    "\n",
    "#### `nn.Dropout(dropout)`\n",
    "\n",
    "`nn.Dropout` is a regularization technique used to prevent overfitting in neural networks. During training, it randomly zeros some of the elements of the input tensor with probability `dropout`, and scales up the remaining elements by `1/(1-dropout)` to maintain the average activation value. During evaluation, `Dropout` does not modify the input and becomes a no-op.\n",
    "\n",
    "- **Parameter**:\n",
    "  - `dropout`: The probability of an element to be zeroed.\n",
    "\n",
    "- **Operation**: Randomly zeroes some of the elements of the input tensor with probability `dropout`.\n",
    "\n",
    "- **Example**:\n",
    "  - If `dropout = 0.2`, then on average 20% of the input elements are set to zero during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    '''\n",
    "    A fully connected feed-forward network, which is is applied to each position separately and identically.\n",
    "    Consists of two linear transformations with ReLU activation between\n",
    "    '''\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float)->None:\n",
    "        '''\n",
    "        d_model: the dim of input and output\n",
    "        d_ff: the dim of inner-layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff,) # W1 and b1\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 and b2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_ff) --> (batch_size, seq_len, d_model)\n",
    "        x = self.linear_1(x) # (batch_size, seq_len, d_model) --> (batch_size, seq_len, d_ff)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5621, 0.2892, 0.1488],\n",
       "          [0.6794, 0.2376, 0.0831],\n",
       "          [0.7727, 0.1837, 0.0437]],\n",
       " \n",
       "         [[0.8420, 0.1360, 0.0220],\n",
       "          [0.8914, 0.0979, 0.0108],\n",
       "          [0.9257, 0.0691, 0.0052]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[-1.1423, -0.6524, -0.8483, -3.1029],\n",
       "          [-1.0977, -0.6293, -0.8202, -2.9807],\n",
       "          [-1.0654, -0.6126, -0.7997, -2.8920]],\n",
       " \n",
       "         [[-1.7739, -0.9792, -1.2479, -4.8360],\n",
       "          [-1.7592, -0.9716, -1.2385, -4.7955],\n",
       "          [-1.7494, -0.9666, -1.2324, -4.7688]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int)->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.Q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.K = nn.Linear(d_model, d_model) # W_k\n",
    "        self.V = nn.Linear(d_model, d_model) # W_v\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        '''\n",
    "        Q = self.Q(x) # (batch_size, seq_len, d_model)\n",
    "        K = self.K(x) # (batch_size, seq_len, d_model)\n",
    "        V = self.V(x) # (batch_size, seq_len, d_model)\n",
    "\n",
    "        attention_score = F.softmax((Q @ K.transpose(1, 2)) / torch.sqrt(torch.tensor(self.d_model)), dim=-1) # (batch_size, seq_len, seq_len)\n",
    "        #print(attention_score)\n",
    "        output = attention_score @ V # (batch_size, seq_len, d_model)\n",
    "        return attention_score, output\n",
    "\n",
    "# Here, batch_size=2, seq_len=3, d_model=4\n",
    "x = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0], [3.0, 4.0, 5.0, 6.0]],\n",
    "                  [[4.0, 5.0, 6.0, 7.0], [5.0, 6.0, 7.0, 8.0], [6.0, 7.0, 8.0, 9.0]]])\n",
    "\n",
    "att = Attention(d_model=4)\n",
    "att(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "Reshaped tensor:\n",
      " tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n",
      "Reshaped tensor with inferred dimension:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor of shape [4, 3]\n",
    "x = torch.arange(12).view(4, 3)\n",
    "print(\"Original tensor:\\n\", x)\n",
    "\n",
    "# Reshape tensor to shape [2, 6] using view\n",
    "y = x.view(2, 6)\n",
    "print(\"Reshaped tensor:\\n\", y)\n",
    "\n",
    "# Reshape with inferred dimension\n",
    "# Here, -1 will be inferred as 4 to keep the total number of elements the same\n",
    "z = x.view(3, -1)\n",
    "print(\"Reshaped tensor with inferred dimension:\\n\", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [2., 3., 4.],\n",
       "         [3., 4., 5.],\n",
       "         [4., 5., 6.]],\n",
       "\n",
       "        [[4., 5., 6.],\n",
       "         [5., 6., 7.],\n",
       "         [6., 7., 8.],\n",
       "         [7., 8., 9.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, batch_size=2, seq_len=3, d_model=4\n",
    "x = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0], [3.0, 4.0, 5.0, 6.0]],\n",
    "                  [[4.0, 5.0, 6.0, 7.0], [5.0, 6.0, 7.0, 8.0], [6.0, 7.0, 8.0, 9.0]]])\n",
    "x.transpose(1,2).contiguous().view(x.shape[0], 4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "a = torch.tensor(0.9).unsqueeze(0)\n",
    "b = torch.tensor(0.9).unsqueeze(0)\n",
    "c = torch.tensor(0.9).unsqueeze(0)\n",
    "x = torch.cat((a,b,c))\n",
    "torch.count_nonzero(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String.format() & Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File doesn't exists.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "a = Path(\"../abc/word_{}.csv\".format(10))\n",
    "if not Path(\"../abc/word_{}.csv\".format(10)).exists():\n",
    "    print(\"File doesn't exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # type: ignore\n",
    "config = {\n",
    "    \"lang_src\": 'en',\n",
    "    \"lang_tgt\": 'it'\n",
    "}\n",
    "\n",
    "ds_raw = load_dataset(path=\"Helsinki-NLP/opus_books\", name=f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train') # 'en-it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32332"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first N samples\n",
    "N = 5000  # Number of samples you want to select\n",
    "subsampled_ds_raw = ds_raw.shuffle(seed=42).select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsampled_ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# from datasets import config # type:ignore\n",
    "# '''\n",
    "# Remove the entire cache directory specified by config.HF_DATASETS_CACHE from the Hugging Face datasets library. \n",
    "# This means it deletes all cached datasets and files stored by the datasets library, \n",
    "# '''\n",
    "# cache_dir = config.HF_DATASETS_CACHE\n",
    "# print(cache_dir)\n",
    "\n",
    "# # Remove the cache directory\n",
    "# shutil.rmtree(cache_dir, ignore_errors=True)\n",
    "\n",
    "# print(f\"Cache directory {cache_dir} removed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yield generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_gen():\n",
    "    i = 0\n",
    "    for item in ds_raw:\n",
    "        if i < 10:\n",
    "            yield item['translation']['en']\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Project Gutenberg\n",
      "Jane Eyre\n",
      "Charlotte Bronte\n"
     ]
    }
   ],
   "source": [
    "gen = sent_gen() \n",
    "print(next(gen))\n",
    "print(next(gen))\n",
    "print(next(gen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "a = (i**2 for i in [1,2,3])\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4,5], dtype=torch.int32)\n",
    "b = torch.tensor([101,1], dtype=torch.int64)\n",
    "c = torch.Tensor([0] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], dtype=torch.int32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1]).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.tensor([[1,2],[2,3]]).dim() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "causal_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a square matrix of size 5x5\n",
    "matrix = torch.ones(5, 5)\n",
    "\n",
    "# Apply torch.triu() to keep the upper triangular part\n",
    "upper_triangular_matrix = (torch.triu(matrix,diagonal=1).to(torch.int32)==0)\n",
    "\n",
    "print(upper_triangular_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor & tensor, (tensor == value), tensor.masked_fill_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5], dtype=torch.int32)\n",
    "b = torch.tensor([101,1], dtype=torch.int64)\n",
    "c = torch.tensor([0] * 5)\n",
    "seq_len = a.size(0) + b.size(0) + c.size(0)\n",
    "\n",
    "print((torch.cat([b,a,c]) !=0).unsqueeze(0).unsqueeze(0).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 12])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((torch.cat([b,a,c]) !=0).unsqueeze(0).unsqueeze(0) & causal_mask(seq_len)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simulating decoder_input != self.pad_token).unsqueeze(0)\n",
    "mask_seq_len = torch.tensor([True, True, False])  # Shape: (3)\n",
    "\n",
    "# Simulating causal_mask(decoder_input.size(0))\n",
    "causal_mask = torch.tensor([[[True, False, False], [True, True, False], [True, True, True]]])  # Shape: (1, 3, 3)\n",
    "\n",
    "attention_score = torch.tensor(range(0, 36)).view(1, 4, 3, 3) # (batch_size, h, seq_len, seq_len)\n",
    "encoder_mask =  mask_seq_len.unsqueeze(0).unsqueeze(0).int() # (1, 1, seq_len)\n",
    "decoder_mask = mask_seq_len.unsqueeze(0) & causal_mask # (1, seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2],\n",
      "          [ 3,  4,  5],\n",
      "          [ 6,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 11],\n",
      "          [12, 13, 14],\n",
      "          [15, 16, 17]],\n",
      "\n",
      "         [[18, 19, 20],\n",
      "          [21, 22, 23],\n",
      "          [24, 25, 26]],\n",
      "\n",
      "         [[27, 28, 29],\n",
      "          [30, 31, 32],\n",
      "          [33, 34, 35]]]])\n"
     ]
    }
   ],
   "source": [
    "print(attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[          0,           1, -1000000000],\n",
      "          [          3,           4, -1000000000],\n",
      "          [          6,           7, -1000000000]],\n",
      "\n",
      "         [[          9,          10, -1000000000],\n",
      "          [         12,          13, -1000000000],\n",
      "          [         15,          16, -1000000000]],\n",
      "\n",
      "         [[         18,          19, -1000000000],\n",
      "          [         21,          22, -1000000000],\n",
      "          [         24,          25, -1000000000]],\n",
      "\n",
      "         [[         27,          28, -1000000000],\n",
      "          [         30,          31, -1000000000],\n",
      "          [         33,          34, -1000000000]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(attention_score.masked_fill_(encoder_mask==0, -1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5849608182907104\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "vocab_size = 5\n",
    "\n",
    "# Simulating model predictions with softmax applied on the last dimension\n",
    "model_prediction = torch.rand(batch_size, seq_len, vocab_size).softmax(dim=-1) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Correcting the labels to be within the valid range [0, vocab_size-1]\n",
    "# Assuming vocab_size = 5, valid indices are 0, 1, 2, 3, and 4\n",
    "label = torch.tensor([2, 3, 0, 4, 1, 0]).view(batch_size, seq_len) # (batch_size, seq_len)\n",
    "\n",
    "# Define the loss function with ignore_index=0\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(model_prediction.view(-1, vocab_size), label.view(-1))\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5849608182907104"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore_index=0 manully, slicing to ignore the 0\n",
    "nn.CrossEntropyLoss()(model_prediction[:,:2,:].reshape(-1, vocab_size), label[:, :2].reshape(-1)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.empty(1,1).fill_(sos_idx).type_as(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.max(tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.5427, 0.1185, 0.6023],\n",
      "        [0.4877, 0.6191, 0.5670, 0.9608]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.6023, 0.9608]),\n",
      "indices=tensor([3, 3]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(2,3,4)[:,-1]\n",
    "print(a)\n",
    "print(torch.max(a,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
